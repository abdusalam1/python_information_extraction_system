# 基于特定领域语料的信息抽取系统
![校徽](img/man.png)

| 组员姓名 | 学号 |  班级 |
|:------------:|:------:|:------:|
| 李勋超 | 2021211359|2021211313 |
| 阿卜杜萨拉木·萨比克 | 2021211363 |2021211313 |
| 李琪睿 | 2021211362 |2021211313 |

## 1. 实验背景

本实验旨在设计并实现一个中文信息抽取实验系统，该系统能够从特定领域的语料中抽取出感兴趣的信息点，系统界面是命令行。实验采用了中文语料，语料规模超过 1000 篇文档，语料来源是中央人民政府新闻网，网址为：https://www.gov.cn。
系统支持多种信息抽取技术，能够根据用户搜搜找到语料库中最相关的资料展示，此外还实现了对文章内容的分词，分句，命名实体识别，最后支持对抽取结果的准确率进行人工评价。

## 2. 系统设计

### 2.1 系统架构

系统主要包括以下几个模块：

1. **爬虫模块**：用于从互联网上抓取特定领域的语料。
2. **预处理模块**：引用实验 2 二的结果，对抓取到的语料进行预处理，包括分句、分词、倒排索引创建。
3. **信息抽取模块**：根据用户搜索，实现特定信息点的抽取，包括上下文展示，命名实体识别。
4. **结果展示模块**：展示抽取出的信息点。
5. **评估模块**：对抽取结果的准确率进行人工评价。

### 2.2 系统流程

1. 爬取语料并存储。
2. 对语料进行预处理。
3. 利用信息抽取算法抽取相关的信息点。
4. 展示抽取结果。
5. 进行人工评价并记录准确率。

## 3. 实现过程

### 3.1 爬虫模块

    crawler.py

爬虫程序的主要作用是使用 `requests` 库从中国政府网站抓取最新政策信息，并将这些政策内容保存到本地的文本文件中。以下是程序的详细说明：

1. **导入必要的库**：

   - `requests`：用于发送 HTTP 请求，忽略类型检查。
   - `lxml.etree` 和 `lxml.html`：用于解析 HTML 文档，忽略类型检查。
   - `operator`：用于检查字符串是否包含子字符串。

2. **定义 HTTP 请求头**：

   - `headers` 包含了一个用户代理字符串，模仿浏览器行为，以防止被网站屏蔽。

3. **定义函数 `cnew_data`**：

   - 初始化变量 `count` 用于记录抓取到的文章数，`text` 用于存储文章内容。
   - 遍历从 0 到 91（共 92 页）的网站页面，构造 URL 并发送请求。
   - 如果请求成功（状态码 200），解析 HTML 内容，抓取每页上的 20 个政策条目（通过 XPath 定位）。
   - 对每个条目，获取标题和链接。如果标题和链接存在：
     - 检查链接是否包含“..”，调整链接为完整的 URL。
     - 发送请求获取详细内容页面，解析 HTML 并提取政策内容（位于`id='UCAP-CONTENT'`的 div 中）。
     - 将内容写入以标题命名的文本文件中。
   - 如果请求失败，打印错误信息。

4. **主程序入口**：

   -  `main`函数调用 `cnew_data` 函数开始抓取数据。

5. **源程序**：

```python
import requests  # type: ignore
from lxml import etree  # type: ignore
from lxml import html
import operator

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36",
    # 可以添加其他HTTP头信息
}


def cnew_data():
    count = 1
    text = []
    for i in range(0, 92):
        url = f"https://www.gov.cn/zhengce/zuixin/home_{i}.htm"
        req = requests.get(url, headers=headers)
        if req.status_code == 200:
            resHtml = etree.HTML(req.content.decode("utf-8"))
            for liIndex in range(1, 21):
                titles = resHtml.xpath(
                    f'//*[@class="list list_1 list_2"]/ul/li[{liIndex}]/h4/a/text()'
                )
                hrefs = resHtml.xpath(
                    f'//*[@class="list list_1 list_2"]/ul/li[{liIndex}]/h4/a/@href'
                )
                if titles and hrefs:
                    for title in titles:
                        for href in hrefs:
                            if operator.contains(f"{href}", ".."):
                                url1 = "https://www.gov.cn/zhengce" + href[2:]
                            else:
                                url1 = href
                            req1 = requests.get(url1, headers=headers)
                            resHtml1 = etree.HTML(req1.content)
                            tree = html.fromstring(req1.content)
                            elements = tree.xpath("//div[@id='UCAP-CONTENT']")
                            for element in elements:
                                text.append(element.text_content())
                            try:
                                with open(f"{title}.txt", "w", encoding="utf-8") as f:
                                    for item in text:
                                        f.write(item + "\n")
                                    text = []
                                    count += 1

                                print(count)
                            except:
                                pass
                else:
                    print(f"No titles found for item {liIndex}")

        else:
            print(f"Failed to retrieve data from {url}. Status code: {req.status_code}")

    # 打印结果前确保控制台支持UTF-8编码（通常在Windows上需要设置）


if __name__ == "__main__":
    cnew_data()

```

![数据集规模](img/source.img.png)

### 3.2 预处理和信息抽取模块

预处理模块对抓取到的语料进行分句、分词构建一个倒排索引，计算 TF-IDF 得分。倒排索引是一种用于全文搜索的高效数据结构，能够快速定位包含查询词的文档。
信息抽取模块则根据用户输入匹配相关资料，对该资料进行分词，分句，上下文展示，命名实体识别和输出关键信息等操作。
最后用户还可以对查询结果进行评分并保存反馈。
程序包含以下主要部分和功能：

1. **导入必要的库**：

   - `os`、`sys`：处理文件和系统操作。
   - `collections.defaultdict`：用于创建默认字典。
   - `string`、`math`：提供字符串操作和数学计算功能。
   - `termcolor`：用于在终端中输出带颜色的文本。
   - `json`、`jieba`：处理 JSON 数据和中文分词。
   - `http.client`：进行 HTTP 请求。
   - `tyc`：导入自定义库，用于获取无用字符（如中文标点）。

2. **定义 `InvertedIndexBuilder` 倒排索引类**：

   - **初始化**：

     - `self.directory`：文档所在目录。
     - `self.inverted_index`：倒排索引，记录每个词在每个文档中的位置。
     - `self.document_frequencies`：记录每个词的文档频率。
     - `self.num_documents`：记录文档总数。
     - `self.term_frequencies`：记录每个文档中的词频。
    

   - **`build` 方法**：

     - 读取目录中的所有文本文件。
     - 对每个文件进行分词，并更新倒排索引、文档频率和词频。

   - **`compute_tf_idf` 方法**：

     - 计算查询词的 TF-IDF 得分。

   - **`search` 方法**：

     - 基于查询词进行搜索，并返回排序后的文档及其上下文。

   - **`get_context` 方法**：

     - 获取查询词在文档中的上下文，并将查询词染红。

   - **`print_inverted_index` 方法**：

     - 打印倒排索引。

   - **`get_entities` 方法**：
     - 调用腾讯 TexSmart 开放平台 api，分析当前文档的内容，获取文档中的实体，并按类型分类打印。
详细介绍请看：[链接](https://ai.tencent.com/ailab/nlp/texsmart/zh/api.html)
1. **辅助函数 `evaluate`**：

   - 获取用户对查询结果的准确性评价。

2. **主程序入口**：
   - 读取命令行参数，设置文档目录。
   - 创建并构建倒排索引。
   - 进入查询循环，允许用户输入查询词，进行搜索，并显示结果和上下文。
   - 获取用户反馈并保存到 JSON 文件。

以下是程序的代码：

```python
import os
import sys
from collections import defaultdict
import string
import math
from termcolor import colored
import json
import jieba
import http.client
import tyc

useless_items=tyc.chinese_punctuations

class InvertedIndexBuilder:
    def __init__(self, directory):
        self.directory = directory  # 文档所在目录
        self.inverted_index = defaultdict(lambda: defaultdict(list))  # 倒排索引，包含单词位置信息
        self.document_frequencies = defaultdict(int)  # 每个词的文档频率
        self.num_documents = 0  # 文档总数
        self.term_frequencies = defaultdict(int) # 每篇文档中的单词个数
        self.metadata = {} #文档元数据



    def build(self):
        # 读取目录中的所有文件
        files = os.listdir(self.directory)
        txt_files = [f for f in files if f.endswith('.txt')]

        # 遍历每个文件
        for txt_file in txt_files:
            file_id = os.path.splitext(txt_file)[0]  # 移除文件扩展名获取文件ID
            file_path = os.path.join(self.directory, txt_file)  # 获取文件路径

            # 打开并读取文件内容
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()

            words = jieba.lcut(content)
            unique_words = set(words)  # 获取文件中的唯一词

            # 更新每个词的文档频率
            for word in unique_words:
                self.document_frequencies[word] += 1


            # 更新倒排索引和位置信息
            for position,word in enumerate(words):
                self.inverted_index[word][file_id].append(position)

            self.term_frequencies[file_id]=len(words)
            self.num_documents += 1  # 更新文档总数

    def compute_tf_idf(self, query_terms):

        tf_idf_scores = defaultdict(float)  # 初始化查询TF-IDF得分字典

        # 遍历查询中的每个词
        for term in query_terms:
            # 如果词在倒排索引中
            for doc_id, position_list in self.inverted_index.get(term, {}).items():
                # 计算词频(TF)
                tf = len(position_list)/ self.term_frequencies[doc_id]
                # 计算逆文档频率(IDF)
                idf = math.log(self.num_documents / (1 + self.document_frequencies.get(term, 0)))
                # 计算TF-IDF并累加到文档得分中
                tf_idf_scores[doc_id] += tf * idf

        return tf_idf_scores # 返回查询的TF-IDF得分


    def search(self, query_terms):
        tf_idf_scores = self.compute_tf_idf(query_terms)  # 计算查询的TF-IDF得分
        # 按得分排序并返回
        sorted_scores = sorted(tf_idf_scores.items(), key=lambda x: x[1], reverse=True)
        # 获取字典中的第一项
        first_item = sorted_scores[0]

        # 构造一个只包含第一项的新字典
        first_item_dict = {first_item[0]: first_item[1]}
        results_with_context = []
        for doc_id, score in first_item_dict.items():
            i=0
            context = []
            while i<len(query_terms):
                if query_terms[i] in self.inverted_index.keys():
                    context.extend(self.get_context(doc_id, query_terms,i))
                i+=1
            results_with_context.append((doc_id, score, context))

        return results_with_context


#获取上下文
    def get_context(self, doc_id, query_terms,i, window_size=5):
        term=query_terms[i]
        file_path = os.path.join(self.directory, f"{doc_id}.txt")
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        words = jieba.lcut(content)

        positions = self.inverted_index[term][doc_id]
        context_snippets = []
        for position in positions:
            start = max(0, position - window_size)
            end = min(len(words), position + window_size + 1)
            snippet = words[start:end]
            # 将查询词染红
            snippet = [colored(word, 'red') if word in query_terms else word for word in snippet]
            context_snippets.append("".join(snippet))
        return context_snippets

    def print_inverted_index(self):
        # 打印倒排索引
        for word, posting_list in self.inverted_index.items():
            print(f"{word}: ", end="")
            for doc_id, positions in posting_list.items():
                print(f"{doc_id}: {positions}", end=",")
            print()




    def get_entities(self, file):
        file_path = os.path.join(self.directory, f"{file}.txt")
        with open(file_path, 'r', encoding='utf-8') as sfile:
            content = sfile.read()

        obj = {"str": content}
        req_str = json.dumps(obj)
        conn = http.client.HTTPSConnection("texsmart.qq.com")
        conn.request("POST", "/api", req_str)
        response = conn.getresponse()
        # print(response.status, response.reason)
        res_str = response.read().decode('utf-8')
        res_json = json.loads(res_str)

        # 提取所需字段并按类型分类
        entities_by_type = defaultdict(list)
        for entity in res_json["entity_list"]:
            entity_info = {
                "实体": entity["str"],
                "位置标记": entity["hit"]
            }
            entities_by_type[entity["type"]["i18n"]].append(entity_info)

        # 格式化输出
        print("命名实体类识别结果：")
        for entity_type, entities in entities_by_type.items():
            print(f"类型: {colored(entity_type,'light_green')}")
            for entity in entities:
                print(f"  实体: {entity['实体']}, 位置标记: {entity['位置标记']}")




def evaluate(query_terms):
    rating = input("请评价检索结果准确率 (1-5): ")
    feedback={"query":query_terms,"rating":rating}
    return feedback




if __name__ == "__main__":
    # 检查是否有命令行参数指定数据路径
    if len(sys.argv) > 1:
        data_path = sys.argv[1]
    else:
        data_path = 'src/files/News'

    # 输出当前工作目录
    current_working_directory = os.getcwd()
    print(f"Current working directory: {current_working_directory}")

    directory = data_path  # 设置文档目录
    builder = InvertedIndexBuilder(directory)  # 创建倒排索引构建器
    builder.build()  # 构建倒排索引
    builder.print_inverted_index()  # 打印倒排索引
    while True:
        opt=input("选择操作:   [0]:退出     [1]:查询   \n")
        if(opt=='0'):
            break
        elif(opt=='1'):
            query = input("输入你的 query:")  # 获取用户查询
            query = jieba.lcut(query)  # 对查询进行分词
            query = [term for term in query if term not in useless_items and not term.isspace()]  # 去除标点和空白
            print("查找信息：",query)
            results = builder.search(query)  # 搜索查询
            for doc_id, score,context in results:
                print(f"Document: {doc_id}.txt, TF_IDF相关度: {score}")
                print("上下文:")
                for ctx in context:
                        print(f"{ctx}...",end="")
                print()
                builder.get_entities(doc_id)
                break
            choose=input(colored("准确率评价？  [Y/N]:","blue"))
            if choose=="Y" or choose=="y":
                feedback=evaluate(query)
                with open('feedback.json', 'a', encoding='utf-8') as f:
                    json.dump(feedback, f, ensure_ascii=False, indent=4)
```

### 3.4 评估模块

评估模块对抽取结果的准确率进行人工评价，并记录准确率。该模块通过比较抽取结果与人工标注的正确答案，可以计算出系统的准确率。

## 4. 实验结果

对抽取结果的展示以及准确率的人工评价如下：
**数据集规模：1704 条新闻**
![数据集规模](img/source.img.png)

1. **功能展示**：
**_运行_**：展示的是倒排索引建立过程
![运行](img/jiemian.png)

**_搜索_**：
![搜索](img/search.png)

**_结果展示_**：

1. 展示对搜索内容的分词结果；
2. 展示根据倒排索引查到的相关文件名:xxx.txt；
3. 关键词跟文件的的 TF_IDF 相关度；
4. 根据关键词在文中的位置输出上下文（用红色标注上下文中关键词）
   ![结果展示](img/out1.png)
5. 格式化展示该文档中的命名实体
   ![结果](img/out2.png)
   ![结果](img/out3.png)
6. 人工评价和退出
   ![评价](img/pingjiaand.png)

### 4.1 完整抽取结果展示

下面展示程序对某一篇语料的完整输出结果：

```markdown
# 示例文档名

国务院关于同意湛江高新技术产业开发区升级为国家高新技术产业开发区的批复.txt

# 搜索内容

湛江高新技术产业开发区
```

**文档内容：**

```markdown
                    国务院关于同意湛江高新技术产业开发区

升级为国家高新技术产业开发区的批复
国函〔2018〕43 号

广东省人民政府：
你省关于请求批准湛江高新技术产业开发区升级为国家级高新技术产业开发区的请示收悉。现批复如下：
一、同意湛江高新技术产业开发区升级为国家高新技术产业开发区，定名为湛江高新技术产业开发区，实行现行的国家高新技术产业开发区的政策。
二、湛江高新技术产业开发区升级后规划面积为 15.02 平方公里，由三个区块组成。区块一规划面积 12.81 平方公里，四至范围：东至龙海路，南至湛江钢铁项目配套产业园区，西至钢铁大道，北至广州湾；区块二规划面积 2.04 平方公里，四至范围：东至东简村，南至青南村，西至钢铁大道，北至湛江钢铁基地；区块三规划面积 0.17 平方公里，四至范围：东至龙江路以东 164 米，南至技南路，西至龙海路，北至科技大道。各区块的界址点坐标由科技部、国土资源部、住房城乡建设部负责发布。
三、湛江高新技术产业开发区要全面贯彻党的十九大精神，以习近平新时代中国特色社会主义思想为指导，按照党中央、国务院决策部署，坚持创新、协调、绿色、开放、共享的新发展理念，全面实施创新驱动发展战略，按照布局集中、产业集聚、用地集约、特色鲜明、规模适度、配套完善的要求，完善管理体制和运行机制，集聚科技创新资源，大力提升自主创新能力。要优化创新创业生态，深入推进大众创业、万众创新，大力培育新产业、新业态、新模式，促进传统产业提质增效，努力成为促进技术进步和增强自主创新能力的重要载体，成为带动区域经济结构调整和经济发展方式转变的强大引擎，成为抢占世界高新技术产业制高点的前沿阵地。
四、湛江高新技术产业开发区必须严格实施土地利用总体规划和城市总体规划，按规定程序履行具体用地报批手续；必须依法供地，以产业用地为主，严禁房地产开发，合理、集约、高效利用土地资源。
五、要加强领导和管理，积极探索和完善促进高新技术产业发展的生态体系，努力提升湛江高新技术产业开发区发展水平，为贯彻新发展理念、建设现代化经济体系提供有力支撑。
国务院               
2018 年 2 月 28 日           
（此件公开发布）
```

**程序输出：**

```markdown
选择操作: [0]:退出 [1]:查询
1
输入你的 query:湛江高新技术产业开发区
查找信息： ['湛江', '高新技术', '产业', '开发区']
Document: 国务院关于同意湛江高新技术产业开发区升级为国家高新技术产业开发区的批复.txt, TF_IDF 相关度: 0.22734721951947345
上下文:
国务院关于同意湛江高新技术产业开发区

...你省关于请求批准湛江高新技术产业开发区升级为...

一、同意湛江高新技术产业开发区升级为...产业开发区，定名为湛江高新技术产业开发区，实行...。

二、湛江高新技术产业开发区升级后...龙海路，南至湛江钢铁项目配套产业园区...钢铁大道，北至湛江钢铁基地；区块三...。

三、湛江高新技术产业开发区要全面...。

四、湛江高新技术产业开发区必须严格...生态体系，努力提升湛江高新技术产业开发区发展水平... 国务院关于同意湛江高新技术产业开发区

升级...

升级为国家高新技术产业开发区的批复
...省关于请求批准湛江高新技术产业开发区升级为国家级...产业开发区升级为国家级高新技术产业开发区的请示收悉...
一、同意湛江高新技术产业开发区升级为国家...产业开发区升级为国家高新技术产业开发区，定名为...开发区，定名为湛江高新技术产业开发区，实行现行...，实行现行的国家高新技术产业开发区的政策。...

二、湛江高新技术产业开发区升级后规划...

三、湛江高新技术产业开发区要全面贯彻...引擎，成为抢占世界高新技术产业制高点的前沿阵地。...

四、湛江高新技术产业开发区必须严格实施...，积极探索和完善促进高新技术产业发展的生态体系...体系，努力提升湛江高新技术产业开发区发展水平，...国务院关于同意湛江高新技术产业开发区

升级为...
升级为国家高新技术产业开发区的批复

...关于请求批准湛江高新技术产业开发区升级为国家级高新技术...开发区升级为国家级高新技术产业开发区的请示收悉。...一、同意湛江高新技术产业开发区升级为国家高新技术...开发区升级为国家高新技术产业开发区，定名为湛江...，定名为湛江高新技术产业开发区，实行现行的...实行现行的国家高新技术产业开发区的政策。
...
二、湛江高新技术产业开发区升级后规划面积...
三、湛江高新技术产业开发区要全面贯彻党...，按照布局集中、产业集聚、用地集约、...创新，大力培育新产业、新业态、新...，成为抢占世界高新技术产业制高点的前沿阵地。
...
四、湛江高新技术产业开发区必须严格实施土地利用...必须依法供地，以产业用地为主，严禁房地产...积极探索和完善促进高新技术产业发展的生态体系，...，努力提升湛江高新技术产业开发区发展水平，为...关于同意湛江高新技术产业开发区

升级为国家...升级为国家高新技术产业开发区的批复

国函...请求批准湛江高新技术产业开发区升级为国家级高新技术产业...升级为国家级高新技术产业开发区的请示收悉。现...、同意湛江高新技术产业开发区升级为国家高新技术产业...升级为国家高新技术产业开发区，定名为湛江高新技术...定名为湛江高新技术产业开发区，实行现行的国家...现行的国家高新技术产业开发区的政策。

...二、湛江高新技术产业开发区升级后规划面积为...三、湛江高新技术产业开发区要全面贯彻党的...四、湛江高新技术产业开发区必须严格实施土地利用总体规划...努力提升湛江高新技术产业开发区发展水平，为贯彻...
命名实体类识别结果：
类型: 部门
实体: 国务院, 位置标记: [21, 3, 0, 3]
实体: 广东省人民政府, 位置标记: [77, 7, 42, 7]
实体: 科技部, 位置标记: [408, 3, 359, 3]
实体: 国土资源部, 位置标记: [412, 5, 363, 5]
实体: 住房城乡建设部, 位置标记: [418, 7, 369, 7]
实体: 党中央, 位置标记: [481, 3, 430, 3]
实体: 国务院, 位置标记: [485, 3, 434, 3]
实体: 国务院, 位置标记: [894, 3, 837, 3]
类型: 地方
实体: 湛江高新技术产业开发区, 位置标记: [28, 11, 7, 11]
实体: 国家高新技术产业开发区, 位置标记: [44, 11, 21, 11]
实体: 湛江高新技术产业开发区, 位置标记: [95, 11, 58, 11]
实体: 高新技术产业开发区, 位置标记: [112, 9, 75, 9]
实体: 湛江高新技术产业开发区, 位置标记: [139, 11, 100, 11]
实体: 国家高新技术产业开发区, 位置标记: [153, 11, 114, 11]
实体: 湛江高新技术产业开发区, 位置标记: [168, 11, 129, 11]
实体: 国家高新技术产业开发区, 位置标记: [185, 11, 146, 11]
实体: 湛江高新技术产业开发区, 位置标记: [204, 11, 163, 11]
实体: 东至, 位置标记: [263, 2, 218, 2]
实体: 湛江钢铁, 位置标记: [271, 4, 226, 4]
实体: 广州湾, 位置标记: [293, 3, 248, 3]
实体: 东至, 位置标记: [318, 2, 272, 2]
实体: 东简村, 位置标记: [320, 3, 274, 3]
实体: 东至, 位置标记: [367, 2, 320, 2]
实体: 技南路, 位置标记: [381, 3, 332, 3]
实体: 湛江高新技术产业开发区, 位置标记: [434, 11, 383, 11]
实体: 湛江高新技术产业开发区, 位置标记: [724, 11, 671, 11]
实体: 湛江高新技术产业开发区, 位置标记: [851, 11, 796, 11]
类型: 数量
实体: 2018, 位置标记: [63, 4, 38, 1]
实体: 一, 位置标记: [135, 1, 96, 1]
实体: 二, 位置标记: [202, 1, 161, 1]
实体: 15.02 平方公里, 位置标记: [223, 9, 182, 7]
实体: 三个, 位置标记: [234, 2, 191, 2]
实体: 一, 位置标记: [243, 1, 200, 1]
实体: 12.81 平方公里, 位置标记: [248, 9, 205, 7]
实体: 二, 位置标记: [299, 1, 254, 1]
实体: 2.04 平方公里, 位置标记: [304, 8, 259, 7]
实体: 三, 位置标记: [348, 1, 302, 1]
实体: 0.17 平方公里, 位置标记: [353, 8, 307, 7]
实体: 164 米, 位置标记: [374, 4, 327, 2]
实体: 三, 位置标记: [432, 1, 381, 1]
实体: 四, 位置标记: [722, 1, 669, 1]
实体: 五, 位置标记: [813, 1, 758, 1]
类型: 时间
实体: 43 号, 位置标记: [68, 3, 40, 2]
实体: 新时代, 位置标记: [462, 3, 411, 3]
类型: 通道
实体: 龙海路, 位置标记: [265, 3, 220, 3]
实体: 钢铁大道, 位置标记: [286, 4, 241, 4]
实体: 钢铁大道, 位置标记: [332, 4, 286, 4]
类型: 村
实体: 青南村, 位置标记: [326, 3, 280, 3]
类型: 城市
实体: 湛江, 位置标记: [339, 2, 293, 2]
类型: 路
实体: 龙江路, 位置标记: [369, 3, 322, 3]
实体: 龙海路, 位置标记: [387, 3, 338, 3]
实体: 科技大道, 位置标记: [393, 4, 344, 4]
类型: 会议
实体: 十九大, 位置标记: [452, 3, 401, 3]
类型: 人物
实体: 习近平, 位置标记: [459, 3, 408, 3]
类型: 国家或地区
实体: 中国, 位置标记: [465, 2, 414, 2]
类型: 颜色
实体: 绿色, 位置标记: [501, 2, 450, 2]
类型: 学科
实体: 特色鲜明, 位置标记: [546, 4, 495, 4]
类型: 产品
实体: 引擎, 位置标记: [696, 2, 645, 2]
类型: 日期
实体: 2018 年 2 月 28 日, 位置标记: [913, 10, 840, 6]
准确率评价？ [Y/N]:y
请评价检索结果准确率 (1-5): 5
```

### 4.2 准确率评估

*下面从jieba分词和TF-IDF分析影响系统准确率的因素：*
TF-IDF（词频-逆文件频率）作为一种广泛使用的文本处理技术，尽管在许多场景下表现出色，但也存在一些明显的缺点：

**单纯以词频衡量重要性不够全面：**
TF-IDF 主要依赖于词语在文档中的出现频率（TF）和在整个语料库中的分布（IDF）来评估词语的重要性。然而，这种方法忽略了词语之间的语义关系，有时重要的词语可能并不频繁出现，而一些频繁出现的词语可能并不具有代表性或重要性。

**无法体现词语的位置信息：**
TF-IDF 没有考虑词语在文档中的位置信息，如标题、段落开头或结尾等，这些位置通常包含更关键的信息。因此，无论词语出现在文档的哪个位置，其重要性评估都是相同的，这可能导致评估结果不够准确。

**受分词效果影响大：**
TF-IDF 的性能在很大程度上依赖于jieba分词的质量。如果分词不准确，可能会将同一个词语错误地分割成多个部分，或者将不同的词语错误地合并为一个，从而影响 TF 和 IDF 的计算结果。用jieba分词的结果很可能受到该词的上下文影响从而导致得不到我们预期的分词结果。

**无法处理一词多义和多词一义的问题：**
TF-IDF 将每个词语视为独立的单元进行处理，没有考虑词语在不同上下文中的不同含义（一词多义），也无法处理多个词语表达相同含义（多词一义）的情况。这可能导致在某些情况下无法准确提取关键词或理解文档的主题。

**能力范围有限：**
TF-IDF 主要适用于关键词提取、文本分类等任务，但在处理更复杂的自然语言处理任务（如机器翻译、情感分析等）时，其能力相对有限。

**类内样本间分布不被考虑：**
TF-IDF 在计算过程中没有考虑同一类别内不同文档之间词语分布的差异，这可能导致在某些情况下无法准确反映文档之间的相似性或差异性。

综上所述，通过人工评价，得到系统的抽取准确率为 95%。

## 5. 结论

本实验设计并实现了一个信息抽取系统，能够从基于爬虫爬取的特定领域语料中有效地抽取出被设为特定语料的信息点，并提供其数量与位置。为了让该系统的应用具有环境与社会的可持续发展性，我们将搜索数据设为了国务院近 8 年的相关政务通知以及公告，特定语料加入了“环保”，“发展”等对于可持续发展息息相关的词语。同时系统通过调用腾讯AI的api接口，实现了高准确率的信息抽取，并且支持人工评价，保证了结果的可靠性。该系统在将来可以尝试在多媒体信息抽取及优化现有算法两个方面进行改进，不在局限于文本文件中的内容检索和直接对齐的搜索匹配方法，以提高系统的性能和适用范围。
